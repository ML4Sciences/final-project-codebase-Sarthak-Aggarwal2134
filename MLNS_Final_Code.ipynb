{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66-uYQaxk1rU",
        "outputId": "134cdcd9-a7ad-4940-8a9e-90f7670e5dee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Using Google Colab for faster computation and GPU utilization\n",
        "# My Google Drive contains all the required files\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR39IJQZfYYP",
        "outputId": "739f83e9-3b8b-49e8-d927-9987c0da3848"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2023.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.22.4)\n",
            "Installing collected packages: rdkit\n",
            "Successfully installed rdkit-2023.3.1\n"
          ]
        }
      ],
      "source": [
        "# Installing dependencies\n",
        "\n",
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JO9clqqYlQAG"
      },
      "outputs": [],
      "source": [
        "# Importing dependencies\n",
        "\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from scipy.spatial import distance_matrix\n",
        "from rdkit.Chem.rdmolops import GetAdjacencyMatrix\n",
        "import pickle as pkl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.utils.data as data_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wrlMeUiwhjty"
      },
      "outputs": [],
      "source": [
        "# Loading train and test keys\n",
        "\n",
        "with open ('drive/MyDrive/keys/train_keys.pkl', 'rb') as fp:\n",
        "  train_keys = pkl.load(fp)\n",
        "with open ('drive/MyDrive/keys/test_keys.pkl', 'rb') as fp:\n",
        "  test_keys = pkl.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZuzRUyjvgfy3"
      },
      "outputs": [],
      "source": [
        "# This cell gives the functions for the one-hot encoded feature vector of the protein molecule\n",
        "\n",
        "def one_of_k_encoding(x, allowable_set):\n",
        "    if x not in allowable_set:\n",
        "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
        "    return list(map(lambda s: x == s, allowable_set))\n",
        "\n",
        "def one_of_k_encoding_unk(x, allowable_set):\n",
        "    if x not in allowable_set:\n",
        "        x = allowable_set[-1]\n",
        "    return list(map(lambda s: x == s, allowable_set))\n",
        "\n",
        "def atom_feature(m, atom_i, i_donor, i_acceptor):\n",
        "    atom = m.GetAtomWithIdx(atom_i)\n",
        "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),\n",
        "                                      ['C', 'N', 'O', 'S', 'F', 'P', 'Cl', 'Br', 'B', 'H']) +\n",
        "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6]) +\n",
        "                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4]) +\n",
        "                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6]) +\n",
        "                    [atom.GetIsAromatic()])    # (10, 7, 5, 7, 1)\n",
        "\n",
        "def get_atom_feature(m):\n",
        "    n = m.GetNumAtoms()\n",
        "    H = []\n",
        "    for i in range(n):\n",
        "        H.append(atom_feature(m, i, None, None))\n",
        "    H = np.array(H)\n",
        "    return H+0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ohlRrbIslrzp"
      },
      "outputs": [],
      "source": [
        "# Creating a Dataset object and custom collate function that can be passed into the DataLoader\n",
        "\n",
        "class Dataset:\n",
        "    def __init__(self, keys):\n",
        "        self.keys = keys\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.keys)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        key = self.keys[index]\n",
        "\n",
        "        mol_w = Chem.MolFromPDBFile('drive/MyDrive/wild_pdb/' + key + '_wild.pdb')\n",
        "        mol_m = Chem.MolFromPDBFile('drive/MyDrive/mutation_pdb/' + key + '_mutation.pdb')\n",
        "        with open('drive/MyDrive/ddg/'+key, 'rb') as f:\n",
        "            labels = pkl.load(f)\n",
        "\n",
        "        # mutation type information\n",
        "        # n_m = mol_m.GetNumAtoms()\n",
        "        # c_m = mol_m.GetConformers()[0]\n",
        "        # P_m = np.array(c_m.GetPositions())  \n",
        "        # adj_m = GetAdjacencyMatrix(mol_m)+np.eye(n_m)\n",
        "        H_m = get_atom_feature(mol_m)\n",
        "\n",
        "        # wild type information\n",
        "        # n_w = mol_w.GetNumAtoms()\n",
        "        # c_w = mol_w.GetConformers()[0]\n",
        "        # P_w = np.array(c_w.GetPositions())  \n",
        "        # adj_w = GetAdjacencyMatrix(mol_w)+np.eye(n_w)\n",
        "        H_w = get_atom_feature(mol_w)\n",
        "        labels=labels\n",
        "        sample = {'H_m': H_m,\\\n",
        "                  'H_w': H_w,\\\n",
        "                  'labels': labels, \\\n",
        "                  'key': key, \\\n",
        "                  }\n",
        "        return sample\n",
        "\n",
        "def collate_fn(batch):\n",
        "    max_natoms_m = 50\n",
        "    max_natoms_w = 50\n",
        "    H_m = np.zeros((len(batch), max_natoms_m, 30))\n",
        "    H_w = np.zeros((len(batch), max_natoms_w, 30))\n",
        "\n",
        "    keys = [] \n",
        "    labels=[]   \n",
        "    for i in range(len(batch)):\n",
        "        natom1 = len(batch[i]['H_m'])\n",
        "        natom2 = len(batch[i]['H_w'])\n",
        "        if (natom1 > 50 or natom2 > 50):\n",
        "          if (natom1 > 50):\n",
        "            natom1 = 50\n",
        "            H_m[i,:natom1] = batch[i]['H_m'][:50]\n",
        "          if(natom2 > 50):\n",
        "            natom2 = 50\n",
        "            H_w[i,:natom2] = batch[i]['H_w'][:50]\n",
        "        else:\n",
        "          H_m[i,:natom1] = batch[i]['H_m']\n",
        "          H_w[i,:natom2] = batch[i]['H_w']\n",
        "        keys.append(batch[i]['key'])\n",
        "        labels.append(batch[i]['labels'])\n",
        "    H_m = torch.from_numpy(H_m).float()\n",
        "    H_w = torch.from_numpy(H_w).float()\n",
        "    return H_m, H_w, labels, keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XTxYf2JchEEZ"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset(train_keys)\n",
        "test_dataset = Dataset(test_keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Hb-drEhbA1u6"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_dataset, 16, shuffle = True, num_workers = 0, collate_fn = collate_fn)\n",
        "test_dataloader = DataLoader(test_dataset, 16, shuffle = True, num_workers = 0, collate_fn = collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLPQ0BQxBTLa",
        "outputId": "972aa3d1-c4f6-4ccc-feeb-054d4ef06782"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 50, 30])\n",
            "torch.Size([16, 50, 30])\n"
          ]
        }
      ],
      "source": [
        "for i_batch, sample in enumerate(train_dataloader):\n",
        "  H_m, H_w, labels, keys = sample\n",
        "  print(H_w.shape)\n",
        "  print(H_m.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Bug2Q9bkWahY"
      },
      "outputs": [],
      "source": [
        "# Code for the Convolutional Autoencoder\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "class CAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CAE, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=50, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(in_channels=16, out_channels=8, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose1d(in_channels=8, out_channels=16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(in_channels=32, out_channels=50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "def train(cae, train_loader, test_loader, criterion, optimizer, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        print(epoch)\n",
        "        running_loss_m = 0.0\n",
        "        running_loss_w = 0.0\n",
        "\n",
        "        # Train on H_m from full dataset\n",
        "        for batch_idx, data in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            H_m, H_w, labels, keys = data\n",
        "            inputs_m = H_m.float().to(device)\n",
        "            print(\"-\", end='')\n",
        "            outputs_m = cae(inputs_m)\n",
        "            loss_m = criterion(outputs_m, inputs_m)\n",
        "            loss_m.backward()\n",
        "            optimizer.step()\n",
        "            running_loss_m += loss_m.item()\n",
        "        for batch_idx, data in enumerate(test_loader):\n",
        "            optimizer.zero_grad()\n",
        "            H_m, H_w, labels, keys = data\n",
        "            inputs_m = H_m.float().to(device)\n",
        "            print(\"-\", end='')\n",
        "            outputs_m = cae(inputs_m)\n",
        "            loss_m = criterion(outputs_m, inputs_m)\n",
        "            loss_m.backward()\n",
        "            optimizer.step()\n",
        "            running_loss_m += loss_m.item()\n",
        "        # Train on H_w from full dataset\n",
        "        for batch_idx, data in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            H_m, H_w, labels, keys = data\n",
        "            inputs_w = H_w.float().to(device)\n",
        "            print(\"+\", end='')\n",
        "            outputs_w = cae(inputs_w)\n",
        "            loss_w = criterion(outputs_w, inputs_w)\n",
        "            loss_w.backward()\n",
        "            optimizer.step()\n",
        "            running_loss_w += loss_w.item()\n",
        "        for batch_idx, data in enumerate(test_loader):\n",
        "            optimizer.zero_grad()\n",
        "            H_m, H_w, labels, keys = data\n",
        "            inputs_w = H_w.float().to(device)\n",
        "            print(\"+\", end='')\n",
        "            outputs_w = cae(inputs_w)\n",
        "            loss_w = criterion(outputs_w, inputs_w)\n",
        "            loss_w.backward()\n",
        "            optimizer.step()\n",
        "            running_loss_w += loss_w.item()\n",
        "        print('Epoch {} Loss_m: {:.6f} Loss_w: {:.6f}'.format(epoch+1, running_loss_m/len(train_loader), running_loss_w/len(train_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3YEvVZ2e43k",
        "outputId": "f67850a9-abdb-4aa7-f2b2-29b06af81cf3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwL8vz6mkNjl",
        "outputId": "3d796edf-6fa7-4ad1-bba0-6c4957b1b558"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "----------------------------------------------------------------------------------------------------------------------------------------++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Epoch 1 Loss_m: 0.064782 Loss_w: 0.051787\n",
            "1\n",
            "----------------------------------------------------------------------------------------------------------------------------------------++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Epoch 2 Loss_m: 0.044347 Loss_w: 0.042368\n",
            "2\n",
            "----------------------------------------------------------------------------------------------------------------------------------------++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Epoch 3 Loss_m: 0.039509 Loss_w: 0.038107\n",
            "3\n",
            "----------------------------------------------------------------------------------------------------------------------------------------++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Epoch 4 Loss_m: 0.035919 Loss_w: 0.034953\n",
            "4\n",
            "----------------------------------------------------------------------------------------------------------------------------------------++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Epoch 5 Loss_m: 0.033438 Loss_w: 0.032853\n",
            "5\n",
            "----------------------------------------------------------------------------------------------------------------------------------------++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Epoch 6 Loss_m: 0.031776 Loss_w: 0.031443\n",
            "6\n",
            "----------------------------------------------------------------------------------------------------------------------------------------++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Epoch 7 Loss_m: 0.030597 Loss_w: 0.030326\n",
            "7\n",
            "----------------------------------------------------------------------------------------------------------------------------------------++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Epoch 8 Loss_m: 0.029684 Loss_w: 0.029403\n",
            "8\n",
            "----------------------------------------------------------------------------------------------------------------------------------------++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Epoch 9 Loss_m: 0.028982 Loss_w: 0.028843\n",
            "9\n",
            "----------------------------------------------------------------------------------------------------------------------------------------++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Epoch 10 Loss_m: 0.028470 Loss_w: 0.028434\n",
            "10\n",
            "----------------------------------------------------------------------------------------------------------------------------------------++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Epoch 11 Loss_m: 0.028175 Loss_w: 0.028148\n",
            "11\n",
            "----------------------------------------------------------------------------------------------------------------------------------------++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Epoch 12 Loss_m: 0.027784 Loss_w: 0.027654\n",
            "12\n",
            "----------------------------------------------------------------------------------------------------------------------------------------++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Epoch 13 Loss_m: 0.027448 Loss_w: 0.027263\n",
            "13\n",
            "----------------------------------------------------------------------------------------------------------------------------------------++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Epoch 14 Loss_m: 0.027239 Loss_w: 0.027044\n",
            "14\n",
            "----------------------------------------------------------------------------------------------------------------------------------------++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Epoch 15 Loss_m: 0.027038 Loss_w: 0.026775\n",
            "15\n",
            "----------------------------------------------------------------------------------------------------------------------------------------++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Epoch 16 Loss_m: 0.026785 Loss_w: 0.026609\n",
            "16\n",
            "----------------------------------------------------------------------------------------------------------------------------------------++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Epoch 17 Loss_m: 0.026620 Loss_w: 0.026539\n",
            "17\n",
            "----------------------------------------------------------------------------------------------------------------------------------------++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Epoch 18 Loss_m: 0.026390 Loss_w: 0.026312\n",
            "18\n",
            "----------------------------------------------------------------------------------------------------------------------------------------++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Epoch 19 Loss_m: 0.026292 Loss_w: 0.026176\n",
            "19\n",
            "----------------------------------------------------------------------------------------------------------------------------------------++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Epoch 20 Loss_m: 0.026180 Loss_w: 0.025992\n"
          ]
        }
      ],
      "source": [
        "# create a CAE instance and set the device\n",
        "cae = CAE()\n",
        "cae = cae.to(device)\n",
        "# define the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(cae.parameters(), lr=0.001)\n",
        "\n",
        "# train the CAE\n",
        "num_epochs = 20\n",
        "train(cae, train_dataloader, test_dataloader, criterion, optimizer, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LatentSpaceDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.data = []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # return (self.data[index][0].view(-1, 8*30), self.data[index][1])\n",
        "        return  self.data[index]\n",
        "\n",
        "    def append(self, x, y):\n",
        "        self.data.append((x, y))"
      ],
      "metadata": {
        "id": "TzVaQQDMJkpd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WrPTsrEF2y2a"
      },
      "outputs": [],
      "source": [
        "# Storing the output of the CAE for the whole dataset into a Latent Space Dataset\n",
        "\n",
        "train_latent_space_dataset = LatentSpaceDataset()\n",
        "test_latent_space_dataset = LatentSpaceDataset()\n",
        "for batch_idx, data in enumerate(train_dataloader):\n",
        "    H_m, H_w, labels, keys = data\n",
        "    M_in = H_m.float().to(device)\n",
        "    M_latent = cae.encoder(M_in)\n",
        "    W_in = H_w.float().to(device)\n",
        "    W_latent = cae.encoder(W_in)\n",
        "    for i in range(len(M_latent)):\n",
        "        diff_latent = M_latent[i] - W_latent[i]\n",
        "        x = diff_latent.detach()\n",
        "        y = labels[i]\n",
        "        train_latent_space_dataset.append(x, y)\n",
        "for batch_idx, data in enumerate(test_dataloader):\n",
        "    H_m, H_w, labels, keys = data\n",
        "    M_in = H_m.float().to(device)\n",
        "    M_latent = cae.encoder(M_in)\n",
        "    W_in = H_w.float().to(device)\n",
        "    W_latent = cae.encoder(W_in)\n",
        "    for i in range(len(M_latent)):\n",
        "        diff_latent = M_latent[i] - W_latent[i]\n",
        "        x = diff_latent.detach()\n",
        "        y = labels[i]\n",
        "        test_latent_space_dataset.append(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_latent_space_dataset))\n",
        "print(len(test_latent_space_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mA5XT-RGoBS",
        "outputId": "fdfc95d9-08a0-4880-9cc9-63fa7a80de9c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1944\n",
            "216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_latent_space_dataset = [(x, torch.FloatTensor([y])) for (x, y) in train_latent_space_dataset]\n",
        "test_latent_space_dataset = [(x, torch.FloatTensor([y])) for (x, y) in test_latent_space_dataset]"
      ],
      "metadata": {
        "id": "gpWXz0BowF4R"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ls_dataloader = DataLoader(train_latent_space_dataset, batch_size=32, shuffle=True)\n",
        "test_ls_dataloader = DataLoader(test_latent_space_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "jY8HbQfwVfA9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for the Dense Neural Network / Fully Connected Neural Network\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(8*30, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 8*30)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "xwSuWBWgYqlf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training parameters\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "# Initialize the network and optimizer\n",
        "net = Net()\n",
        "net = net.to(device)\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "# Define the loss function as Mean Squared Error (MSE) loss\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Run the training loop for a specified number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    list1_train = []\n",
        "    list2_train = []\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_ls_dataloader):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs.to(device))\n",
        "        loss = criterion(outputs, labels.to(device))\n",
        "        labels = labels.data.cpu().numpy()\n",
        "        outputs = outputs.detach().data.cpu().numpy()\n",
        "        list1_train = np.append(list1_train, labels)\n",
        "        list2_train = np.append(list2_train, outputs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    rp_train = np.corrcoef(list2_train, list1_train)[0,1]\n",
        "    # x = np.array(list1_test).reshape(-1,1)\n",
        "    # y = np.array(list2_test).reshape(-1,1)\n",
        "    # rmse = np.sqrt(((y - x) ** 2).mean())\n",
        "    # Print the average loss for the epoch\n",
        "    print('Epoch [%d/%d], Loss: %.4f, PCC_Train: %.4f' % (epoch+1, num_epochs, running_loss/len(train_ls_dataloader), rp_train))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxk0s0ktvQQf",
        "outputId": "449dadcb-057a-4f31-9bca-53781b144b1e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 2.1599, PCC_Train: 0.2093\n",
            "Epoch [2/10], Loss: 1.7841, PCC_Train: 0.4387\n",
            "Epoch [3/10], Loss: 1.6038, PCC_Train: 0.5263\n",
            "Epoch [4/10], Loss: 1.4750, PCC_Train: 0.5774\n",
            "Epoch [5/10], Loss: 1.3571, PCC_Train: 0.6214\n",
            "Epoch [6/10], Loss: 1.2013, PCC_Train: 0.6765\n",
            "Epoch [7/10], Loss: 1.1082, PCC_Train: 0.7059\n",
            "Epoch [8/10], Loss: 1.0363, PCC_Train: 0.7299\n",
            "Epoch [9/10], Loss: 0.9061, PCC_Train: 0.7686\n",
            "Epoch [10/10], Loss: 0.8249, PCC_Train: 0.7918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the Pearson Correlation Coefficient between the actual and predicted value and the RMSE of the predictions from the actual value\n",
        "\n",
        "list1_test = []\n",
        "list2_test = []\n",
        "for batch_idx, data in enumerate(test_ls_dataloader):\n",
        "    inputs, labels = data\n",
        "    outputs = net(inputs.to(device))\n",
        "    labels = labels.data.cpu().numpy()\n",
        "    outputs = outputs.detach().data.cpu().numpy()\n",
        "    list1_test = np.append(list1_test, labels)\n",
        "    list2_test = np.append(list2_test, outputs)\n",
        "rp_test = np.corrcoef(list2_test, list1_test)[0,1]\n",
        "x = np.array(list1_test).reshape(-1,1)\n",
        "y = np.array(list2_test).reshape(-1,1)\n",
        "rmse = np.sqrt(((y - x) ** 2).mean())\n",
        "print('PCC_Test: %.4f, RMSE: %.4f' % (rp_test, rmse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Axftlgj9DRC5",
        "outputId": "5f9ce50c-1127-4aad-b1aa-06cd203804bf"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCC_Test: 0.5353, RMSE: 1.1877\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}